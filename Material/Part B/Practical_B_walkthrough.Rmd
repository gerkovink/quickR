---
title: "Practical B"
author: "Gerko Vink"
date: "Statistical Programming with R"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
  
<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>
  
---

In this practical I detail multiple skills and show you a workflow for (predictive) analytics. 

Feel free to ask me, if you have questions. 

All the best, 

Gerko

---

# Exercises

---

The following packages are required for this practical:
```{r, message=FALSE}
library(dplyr)
library(magrittr)
library(mice)
library(ggplot2)
library(MASS)
library(class)
library(mvtnorm)
```

---

The data sets `elastic1` and `elastic2` from the package `DAAG` were obtained
using the same apparatus, including the same rubber band, as the data frame
`elasticband`. Package `DAAG` has since been archived and the elastic 2 data can be obtained as:
```{r}
connection <- url("https://www.gerkovink.com/quickR/elasticdata.RData")
load(connection)
```


1. **Using a different symbol and/or a different color, plot the data
from the two data frames `elastic1` and `elastic2` on the same graph. Do the two
sets of results appear consistent?**

```{r}
elastic <- rbind(elastic1, elastic2)
elastic$source <- c(rep("Elastic1", nrow(elastic1)), 
                    rep("Elastic2", nrow(elastic2)))

elastic %>%
  ggplot(aes(stretch, distance, colour = source)) +
  geom_point() + 
  geom_smooth(method = "lm")

```

The results seem very consistent: Data set `elastic2` has more observations over a larger range, but both sets result in roughly the same regression line. Data set `elastic1` seems to have an *odd-one-out* value.

---

2. **For each of the data sets `elastic1` and `elastic2`, determine the regression of
distance on stretch. In each case determine:**

- fitted values and standard errors of fitted values and
- the $R^2$ statistic.

Compare the two sets of results. What is the key difference between the two sets
of data?

First we run the two models:
```{r}
fit1 <- 
  elastic1 %$%
  lm(distance ~ stretch)

fit2 <- 
  elastic2 %$%
  lm(distance ~ stretch)
```

and then we compare the fitted values
```{r}
fit1 %>% predict(se.fit = TRUE)
fit2 %>% predict(se.fit = TRUE)
```
We see that `fit1` (based on `elastic1`) has a larger residual standard deviation (i.e. `$residual.scale`). 

To get the $R^2$ we can run a summary on the fitted models:
```{r}
fit1 %>% summary()
fit2 %>% summary()
```

Or we can grab the $R^2$ directly from the object without a pipe
```{r}
summary(fit1)$r.squared
summary(fit2)$r.squared
```
The model based on `elastic2` has smaller standard errors and a much larger $R^2$.
This is due to the larger range of values in `elastic2`, and the absence of an outlier.

---

3. **Study the *residual vs leverage* plots for both models. Hint use `plot()` on the fitted object**

```{r}
fit1 %>% plot(which = 5) #the fifth plot is the residuals vs leverage plot
fit2 %>% plot(which = 5)
```

For `elastic1`, case 2 has the largest influence on the estimation. However, it is not the case with the largest residual:
```{r}
fit1$residuals
```

As we can see, case 7 has the largest residual.

---

Because there is a single value that influences the estimation and is somewhat different than the other values, a robust form of regression may be advisable to obtain more stable estimates. When robust methods are used, we refrain from omitting a suspected outlier from our analysis. In general, with robust analysis, influential cases that are not conform the other cases receive less weight in the estimation procedure then under non-robust analysis.

---

4. **Use the robust regression function `rlm()` from the `MASS` package to fit lines to the data in `elastic1` and `elastic2`. Compare the results with those from use of `lm()`:**

- residuals
- regression coefficients, 
- standard errors of coefficients, 
- plots of residuals against fitted values.

First, we run the same models again with `rlm()`
```{r}
fit1.rlm <- 
  elastic1 %$%
  rlm(distance ~ stretch)

fit2.rlm <- 
  elastic2 %$%
  rlm(distance ~ stretch)
```

and then we look at the coefficients and the residuals
```{r}
data.frame(lm = coef(fit1), 
           rlm = coef(fit1.rlm))

data.frame(lm = coef(fit2), 
           rlm = coef(fit2.rlm))
```

We see that the coefficients for `elastic1` are different for `lm()` and `rlm()`. The coefficients for `elastic2` are very similar. 

To study the standard errors of the coefficients:
```{r}
data.frame(lm = summary(fit1)$coefficients[, "Std. Error"], 
           rlm = summary(fit1.rlm)$coefficients[, "Std. Error"])

data.frame(lm = summary(fit2)$coefficients[, "Std. Error"], 
           rlm = summary(fit2.rlm)$coefficients[, "Std. Error"])
```

The standard errors for the estimates for `elastic1` have become much smaller with `rlm()` compared to standard `lm()` estimation. The standard errors for `elastic2` are very similar. 

To study the residuals:
```{r}
data.frame(lm = residuals(fit1), 
           rlm = residuals(fit1.rlm))

data.frame(lm = residuals(fit2), 
           rlm = residuals(fit2.rlm))
```

The residual trend for both models is very similar. Remember that different values will still be different under robust analyses; they are only given less influence. 

---

To plot the residuals against the fitted values:
```{r}
plot(fit1, which = 1, add.smooth = "FALSE", col = "blue", main = "elastic1")
points(residuals(fit1.rlm) ~ fitted(fit1.rlm), col = "orange")

plot(fit2, which = 1, add.smooth = "FALSE", col = "blue", main = "elastic2")
points(residuals(fit2.rlm) ~ fitted(fit2.rlm), col = "orange")
```

The case 2 residual in elastic1 is smaller in the robust regression. This is
because the case had less weight in the `rlm()` estimation of the coefficients than
in the ordinary `lm()` regression.


---

5. **Use the `elastic2` variable `stretch` to obtain predictions on the model fitted on `elastic1`.**

```{r}
pred <- predict.lm(fit1, newdata = data.frame(stretch = elastic2$stretch))
```

---

6. **Now make a scatterplot to investigate similarity between plot the predicted values against the observed values for `elastic2`**
```{r}
new.dat <- data.frame(stretch = elastic2$stretch, 
                      distance = c(elastic2$distance, pred))

new.dat$source <- c(rep("original", nrow(elastic2)), 
                    rep("predicted", nrow(elastic2)))

new.dat %>%
  ggplot(aes(stretch, distance, colour = source)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

The predicted values are very similar to the observed values:
```{r}
data.frame(distance = elastic2$distance, predicted = pred) %>%
  ggplot(aes(distance, predicted)) + 
  geom_point()
  
```

They do not strictly follow the straight line because there is some modeling error: we use `elastic1`'s model to predict `elastic2`'s distance [error source 1] and we compare those predictions to `elastic2`'s observed distance [error source 2]. However, if you consider the modeling, these predictions are very accurate and have high correlations with the observed values:
```{r}
data.frame(distance = elastic2$distance, predicted = pred) %>%
  cor() 
```

---

The mammalsleep dataset is part of `mice`. It contains the Allison and Cicchetti (1976) data for mammalian species. To learn more about this data, type
```{r}
?mammalsleep
```

--- 

7. **Fit and inspect a model where `brw` is modeled from `bw`**
```{r}
mammalsleep %$%
  lm(brw ~ bw) %>%
  anova()
```
It seems that we can model brain weight `brw` with body weight `bw`. If we inspect the linear model, we see that the $R^2$ is quite high:
```{r}
mammalsleep %$%
  lm(brw ~ bw) %>%
  summary()
```

---

8. **Now fit and inspect a model where `brw` is predicted from both `bw` and `species`**
```{r}
mammalsleep %$%
  lm(brw ~ bw + species) %>%
  anova()
```

There seems to be a perfect fit and we don't get any p-values. If we inspect the linear model `summary()`, we find that every animal only is observed once. Adding species as a predictor yields the most overfitted model we may obtain and our residuals drop effectively to zero. 

```{r}
mammalsleep %$%
  lm(brw ~ bw + species) %>%
  summary()
```

The analysis we ran is in fact equivalent to running a fixed effects model on clusters of size 1. Since in that scenario there is no clustering, we should omit the fixed effect for `species` and just model the random variation (in this case done by the residual variance).

---

9. **Can you find a model that improves the $R^2$ in modeling `brw`?**

Since we're considering linear models so far, I limit myself to linear models only. The basis of the linear model is the variance covariance matrix and how it translates to data relations. This is most easily linearly summarized in the correlation matrix:
```{r}
mammalsleep %>%
  subset(select = -species) %>% #exclude factor species
  cor(use = "pairwise.complete.obs") #pairwise deletion
```

This matrix contains quite a few cells. To obtain only the correlations with `brw` we could select the respective column:
```{r}
mammalsleep %>%
  subset(select = -species) %>% #exclude factor species
  cor(use = "pairwise.complete.obs") %>% #pairwise deletion
  subset(select = brw) #only column brw from the correlation matrix
```

It seems that the following variables have a rather nice relation with `brw`:

- `sws`: short wave sleep
- `ts` : total sleep
- `mls`: maximum life span
- `gt` : gestation time
- `sei`: sleep exposure index

However, from the larger correlation matrix we can also see that `ts` is highly colinear with `sws` - in fact, `ts` is calculated as the sum over `sws` and `ps`. Including both variables will not hurt our $R^2$ per se, but it will certainly trouble the precision of our estimation as including both variables will yield much larger standard errors. It may be wise to select `sws` as a predictor: `ts` contains a source of *error* in the form of `ps`, so its linear association with `brw` is slightly weaker. However, `sws` misses 14 cases and `ts` misses only 4. 
```{r}
mammalsleep %>%
  summary()
```

Therefore it is highly preferable to use `ts` in the model, despite its weaker association.

We run the new model:
```{r}
fit <- 
  mammalsleep %$%
  lm(brw ~ bw + ts + mls + gt + sei)
  
fit %>%
  summary()
```
and we have obtained a very high $R^2$. If prediction was our goal, we are doing great.

---

10. **Inspect the diagnostic plots for the model obtained in `exercise 9`. What issues can you detect?**
```{r}
fit %>%
  plot(which = 1:6)
```

Some issues spring to mind:

- There error variance seems to be heteroscedastic [but we have a rather small sample]
- The residuals are not normally distributed in the extreme tails
- The following case has a large leverage: 1
- The following case has large residual: 5
```{r}
mammalsleep$species[c(1, 5)]
```

If we sort the `brw` variable together with its strongest predictor 
```{r}
mammalsleep %>% 
  subset(select = c(species, brw, bw, ts, mls, gt, sei)) %>%
  arrange(desc(brw)) #sort the data in descending order based on brw
```

we see that `Man` has a large `brw` for small `bw` and that `African elephant` is so large that it massively influences the model. For `Man` we would expect a much lighter brain given its body weight. We can also see this from the residuals:
```{r}
fit %>%
  residuals()
```

from the influence statistics:
```{r}
fit %>%
  influence()
```
From the influence we see:

- the residual standard deviation `$sigma` would drop when the first case and the fifth case would be omitted.
- the coefficients `$coefficients$ would dramatically change if cases 1 and 5 were omitted

The `influence(fit)$coefficients` is equivalent to the `dfbeta()` return:
```{r}
head(influence(fit)$coefficients)

fit %>%
  dfbeta() %>%
  head()
```

---

11. **Reproduce the clustering data from the slides twice, but now instead of $\pm 1.5$ use an adjustment of $\pm .5$ and $\pm 2.5$, respectively**

First we create the data:
```{r}
set.seed(123)
sigma <- matrix(c(1, .5, .5, 1), 2, 2)
sim.data <- rmvnorm(n = 100, 
                    mean = c(5, 5), 
                    sigma = sigma)
colnames(sim.data) <- c("x1", "x2")
```
add some clustering
```{r}
sim.data <- 
  sim.data %>%
  as_tibble %>%
  mutate(class = sample(c("A", "B", "C"), size = 100, replace = TRUE))
```
and add the adjustments to the data
```{r}
sim.data.half <- 
  sim.data %>%
  mutate(x2 = case_when(class == "A" ~ x2 + .5,
                        class == "B" ~ x2 - .5,
                        class == "C" ~ x2 + .5),
         x1 = case_when(class == "A" ~ x1 - .5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + .5))
sim.data.twohalf <- 
  sim.data %>%
  mutate(x2 = case_when(class == "A" ~ x2 + 2.5,
                        class == "B" ~ x2 - 2.5,
                        class == "C" ~ x2 + 2.5),
         x1 = case_when(class == "A" ~ x1 - 2.5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + 2.5))
```

---

12. **Add a column to the data sets that indicates a `Train` (25%) and a `Test` (75%) part.**
```{r}
sim.data.half %<>% 
  mutate(set = sample(c("Train", "Test"), size=100, prob = c(.25, .75), replace=TRUE))
sim.data.twohalf %<>% 
  mutate(set = sample(c("Train", "Test"), size=100, prob = c(.25, .75), replace=TRUE))
```

---

13. **Fit the `K-NN` model to both data sets. Use `k = 3`.**
For the first model with the `.5` adjustment:
```{r}
# create training and test
train.half <- subset(sim.data.half, set == "Train", select = c(x1, x2))
class.half <- subset(sim.data.half, set == "Train", select = class)
test.half <- subset(sim.data.half, set == "Test", select = c(x1, x2))

#run k-nn model
fit.knn.half <- knn(train = train.half,
                    test = test.half,
                    cl = as.matrix(class.half),
                    k = 3)
```
Then for the model with the `2.5` adjustment:
```{r}
# create training and test
train.twohalf <- subset(sim.data.twohalf, set == "Train", select = c(x1, x2))
class.twohalf <- subset(sim.data.twohalf, set == "Train", select = class)
test.twohalf <- subset(sim.data.twohalf, set == "Test", select = c(x1, x2))

#run k-nn model
fit.knn.twohalf <- knn(train = train.twohalf,
                       test = test.twohalf,
                       cl = as.matrix(class.twohalf),
                       k = 3)
```

---

14. **What is the percentage of correct predictions for each model?**
For the `.5` adjustment data:
```{r}
class.test.half <- subset(sim.data.half, set == "Test", select = class) %>%
  as.matrix()
correct.half <- fit.knn.half == class.test.half
mean(correct.half)
```

and for the `2.5` adjustment data:
```{r}
class.test.twohalf <- subset(sim.data.twohalf, set == "Test", select = class) %>%
  as.matrix()
correct.twohalf <- fit.knn.twohalf == class.test.twohalf

mean(correct.twohalf)
```

The model based on the 2.5 adjustment data performs much better. But in this model the classes are also more separated. 

---

15. **Plot the false and correct predictions for both models.**
For the `.5` adjustment data:
```{r}
cbind(test.half, correct.half) %>%
  ggplot(aes(x1, x2,  colour = correct.half)) +
  geom_point() +
  scale_colour_manual(values = c("red", "black")) + 
  ggtitle("K-NN classification \n Adjustment = .5")
```

We can see many mistakes for this model, but then again; there is not much clustering of values to detect

For the `2.5` adjustment data this changes:
```{r}
cbind(test.twohalf, correct.twohalf) %>%
  ggplot(aes(x1, x2,  colour = correct.twohalf)) +
  geom_point() +
  scale_colour_manual(values = c("red", "black")) + 
  ggtitle("K-NN classification \n Adjustment = 2.5")
```
The clusters are visisbly separated. It is quite difficult to misclassify values based on their three closest neighbors - except for the values that are somewhat in between two (or more) clusters. Now we only make 1 misclassification given that we have `set.seed(123)` - different seeds may yield different data and, hence, different results. The misclassified value is indeed one of those values: in between two clusters.

---

16. **Write a function that determines the optimum `k` with respect to classification error. Have the function return the following:**

- The optimum `k` (i.e. the lowest `k` with the most correct predictions)
- The proportion correctly predicted for optimum `k`
- A data frame with the proportion correct for every `k`

```{r}
Optimize.knn <- function(train.set, test.set, train.class, test.class, min = 1, max = NULL) {
  if (is.null(max)) {
    max <- nrow(train.set)
  }
  if (!is.matrix(train.class)) {
    train.class <- as.matrix(train.class)
  }
  output <- list() #object to store in
  for (i in min:max){
    output[[i]] <- knn(train = train.set,
                       test = test.set,
                       cl = train.class,
                       k = i)
  }
  compare <- function(x) mean(x == test.class)
  correct <- data.frame(k = 1:max,
                        p.correct = sapply(output, compare))
  optimum <- min(correct$k[which.max(correct$p.correct)])
  result <- list(optimum.k = optimum,
                 max.p.correct = max(correct$p.correct),
                 results = correct)
  return(result)
}
```

---

17. **Execute your function twice: once for the data set based on the `.5` adjustment and once for the data set based on the `2.5` adjustment. Does the previously used `k=3` yield the optimal classification prediction?**
```{r}
Optimize.knn(train.half, test.half, class.half, class.test.half)
Optimize.knn(train.twohalf, test.twohalf, class.twohalf, class.test.twohalf)
```

Please note that any ties are solved on a random basis. That is why running the same code again, *slightly* different results may be obtained. 
```{r}
Optimize.knn(train.twohalf, test.twohalf, class.twohalf, class.test.twohalf)
```

---

# Additional Exercises 

---

For those who have already finished the exercises, I have prepared some more complicated modeling exercises. These exercises use the `chisq.test()` and `fisher.test()` functions to test contingency tables. We can also use these techniques in modeling efforts. See the below exercises. 

---

A recruiter for a large company suspects that the process his company uses to hire new applicants is biased. To test this, he records the application numbers that have been successfully hired in the last hiring round. He finds the following pattern:
```{r}
numbers <- data.frame(hired = c(11, 19, 13, 4, 8, 4),
                      not_hired = c(89, 81, 87, 96, 92, 11))
numbers$probability <- round(with(numbers, hired / (hired + not_hired)), 2)
rownames(numbers) <- c(paste("Application number starts with", 0:5))
numbers
```

---

18. **Investigate whether there is indeed a pattern: does the probability to be hired depend a posteriori on the job application number?**

```{r}
chisq <- chisq.test(numbers[, 1:2])
chisq$expected
```

Expected cell frequencies are very low for starting number `5` Let's run a Fisher exact test:
```{r}
fisher.test(numbers[, 1:2])
```

There seems to be a pattern: lower starting values for the application number have a higher probability of being hired. It seems that the difference in probability to be hired is very unlikely to occur given the assumption of independence. 

---

19. **The researcher knows that application numbers are assigned to new applications based on the time and date the application has been submitted. A colleague suggests that applicants who submit early on in the process tend to be better prepared than applicants who submit later on in the process. Test this assumption by running a $X^2$ test to compare the original data to the following pattern where a 2-percent drop over the starting numbers is expected.**

```{r}
decreasing <- data.frame(hired = c(16, 14, 12, 10, 8, 1),
                         not_hired = c(84, 86, 88, 91, 93, 14))
decreasing$probability <- round(with(decreasing, hired / (hired + not_hired)), 2)
decreasing
```

To run this data, we can compute the chi-squared test by hand:
```{r}
chisq.decreasing <- sum((numbers[, 1:2] - decreasing[, 1:2])^2 / (decreasing[, 1:2]))
chisq.decreasing
```
The chi-squared value for this test is `r chisq.decreasing`. The corresponding p-value is 
```{r}
1 - pchisq(chisq.decreasing, df = 5)
```
Not succesful: the decreasing probability model does not fit to the data the researcher has observed. 

---

The board of the company would like to improve their process if the process is systematically biased. They tell the recruiter that their standard process in hiring people is as follows: 

1. The secretary sorts the applications by application number
2. The board determines for every application if the applicant would be hired
3. If half the vacancies are filled they take a coffee break
4. After the coffee break they continue the same process to distribute the other applications over the remaining vacancies. 

The recruiter suspects that the following psychological process is occuring: The board realized at the coffee break that they were running out of vacancies to award the remaining half of the applications, then became more conservative for a while and return to baseline in the end. 

If that were true, the following expected cell frequencies might be observed:
```{r}
oops <- data.frame(hired = c(14, 14, 14, 2, 12, 3),
                   not_hired = c(86, 86, 86, 98, 88, 12))
oops$probability <- round(with(oops, hired / (hired + not_hired)), 2)
oops
```

---

20. **Verify if the `oops` pattern would fit to the observed pattern from the `numbers` data. Again, use a chi-squared test.**

To run this data, we can compute the chi-squared test by hand:
```{r}
chisq.oops <- sum((numbers[, 1:2] - oops[, 1:2])^2 / (oops [, 1:2]))
chisq.oops
```

The chi-squared value for this test is `r chisq.oops`. The corresponding p-value is 
```{r}
1 - pchisq(chisq.oops, df = 5)
```
This model seems to fit to the observations. 

---

21. **Plot the probability against the starting numbers and use different colours for each of the following patterns:** 

- the observed pattern
- the independence pattern (equal probability)
- the `decreasing` probability pattern
- the `oops` pattern.

```{r warning=FALSE, message=FALSE}
plotdata <- data.frame(count = c(numbers$hired,
                                 chisq$expected[, 1], 
                                 decreasing$hired,
                                 oops$hired),
                       total = rep(c(rep(100, 5), 15), 4),
                       start.nr = rep(0:5, 4),
                       model = rep(c("observed", 
                                     "independence",
                                     "decreasing", 
                                     "oops"),
                                   rep(6, 4)))
plotdata
plotdata %>%
  mutate(proportion = count / total) %>%
  ggplot(aes(start.nr, proportion, colour = model)) + 
  geom_point() + 
  geom_smooth()
```

---

End of Practical


