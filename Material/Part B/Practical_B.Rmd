---
title: "Practical B"
author: "Gerko Vink"
date: ""
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
  
<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>
  
---

In this practical I detail multiple skills and show you a workflow for (predictive) analytics. 

Feel free to ask me, if you have questions. 

All the best, 

Gerko

---

# Exercises

---

The following packages are required for this practical:
```{r, message=FALSE}
library(dplyr)
library(magrittr)
library(mice)
library(ggplot2)
library(MASS)
library(class)
library(mvtnorm)
```

---

The data sets `elastic1` and `elastic2` from the package `DAAG` were obtained
using the same apparatus, including the same rubber band, as the data frame
`elasticband`. Package `DAAG` has since been archived and the elastic 2 data can be obtained as:
```{r}
connection <- url("https://www.gerkovink.com/quickR/elasticdata.RData")
load(connection)
```


1. **Using a different symbol and/or a different color, plot the data
from the two data frames `elastic1` and `elastic2` on the same graph. Do the two
sets of results appear consistent?**

---

2. **For each of the data sets `elastic1` and `elastic2`, determine the regression of
distance on stretch. In each case determine:**

- fitted values and standard errors of fitted values and
- the $R^2$ statistic.

Compare the two sets of results. What is the key difference between the two sets
of data?

---

3. **Study the *residual vs leverage* plots for both models. Hint use `plot()` on the fitted object**

---

Because there is a single value that influences the estimation and is somewhat different than the other values, a robust form of regression may be advisable to obtain more stable estimates. When robust methods are used, we refrain from omitting a suspected outlier from our analysis. In general, with robust analysis, influential cases that are not conform the other cases receive less weight in the estimation procedure then under non-robust analysis.

---

4. **Use the robust regression function `rlm()` from the `MASS` package to fit lines to the data in `elastic1` and `elastic2`. Compare the results with those from use of `lm()`:**

- residuals
- regression coefficients, 
- standard errors of coefficients, 
- plots of residuals against fitted values.


---

5. **Use the `elastic2` variable `stretch` to obtain predictions on the model fitted on `elastic1`.**

---

6. **Now make a scatterplot to investigate similarity between plot the predicted values against the observed values for `elastic2`**

---

The mammalsleep dataset is part of `mice`. It contains the Allison and Cicchetti (1976) data for mammalian species. To learn more about this data, type
```{r eval = FALSE}
?mammalsleep
```

--- 

7. **Fit and inspect a model where `brw` is modeled from `bw`**

---

8. **Now fit and inspect a model where `brw` is predicted from both `bw` and `species`**

---

9. **Can you find a model that improves the $R^2$ in modeling `brw`?**

---

10. **Inspect the diagnostic plots for the model obtained in `exercise 9`. What issues can you detect?**

---

11. **Reproduce the clustering data from the slides twice, but now instead of $\pm 1.5$ use an adjustment of $\pm .5$ and $\pm 2.5$, respectively**

---

12. **Add a column to the data sets that indicates a `Train` (25%) and a `Test` (75%) part.**

---

13. **Fit the `K-NN` model to both data sets. Use `k = 3`.**

---

15. **Plot the false and correct predictions for both models.**

---

16. **Write a function that determines the optimum `k` with respect to classification error. Have the function return the following:**

---

17. **Execute your function twice: once for the data set based on the `.5` adjustment and once for the data set based on the `2.5` adjustment. Does the previously used `k=3` yield the optimal classification prediction?**

---

# Additional Exercises 

---

For those who have already finished the exercises, I have prepared some more complicated modeling exercises. These exercises use the `chisq.test()` and `fisher.test()` functions to test contingency tables. We can also use these techniques in modeling efforts. See the below exercises. 

---

A recruiter for a large company suspects that the process his company uses to hire new applicants is biased. To test this, he records the application numbers that have been successfully hired in the last hiring round. He finds the following pattern:
```{r}
numbers <- data.frame(hired = c(11, 19, 13, 4, 8, 4),
                      not_hired = c(89, 81, 87, 96, 92, 11))
numbers$probability <- round(with(numbers, hired / (hired + not_hired)), 2)
rownames(numbers) <- c(paste("Application number starts with", 0:5))
numbers
```

---

18. **Investigate whether there is indeed a pattern: does the probability to be hired depend a posteriori on the job application number?**

---

19. **The researcher knows that application numbers are assigned to new applications based on the time and date the application has been submitted. A colleague suggests that applicants who submit early on in the process tend to be better prepared than applicants who submit later on in the process. Test this assumption by running a $X^2$ test to compare the original data to the following pattern where a 2-percent drop over the starting numbers is expected.**

```{r}
decreasing <- data.frame(hired = c(16, 14, 12, 10, 8, 1),
                         not_hired = c(84, 86, 88, 91, 93, 14))
decreasing$probability <- round(with(decreasing, hired / (hired + not_hired)), 2)
decreasing
```

---

The board of the company would like to improve their process if the process is systematically biased. They tell the recruiter that their standard process in hiring people is as follows: 

1. The secretary sorts the applications by application number
2. The board determines for every application if the applicant would be hired
3. If half the vacancies are filled they take a coffee break
4. After the coffee break they continue the same process to distribute the other applications over the remaining vacancies. 

The recruiter suspects that the following psychological process is occuring: The board realized at the coffee break that they were running out of vacancies to award the remaining half of the applications, then became more conservative for a while and return to baseline in the end. 

If that were true, the following expected cell frequencies might be observed:
```{r}
oops <- data.frame(hired = c(14, 14, 14, 2, 12, 3),
                   not_hired = c(86, 86, 86, 98, 88, 12))
oops$probability <- round(with(oops, hired / (hired + not_hired)), 2)
oops
```

---

20. **Verify if the `oops` pattern would fit to the observed pattern from the `numbers` data. Again, use a chi-squared test.**

---

21. **Plot the probability against the starting numbers and use different colours for each of the following patterns:** 

- the observed pattern
- the independence pattern (equal probability)
- the `decreasing` probability pattern
- the `oops` pattern.

---

End of Practical


