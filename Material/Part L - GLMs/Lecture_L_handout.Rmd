---
title: "Generalized linear models"
author: "Gerko Vink"
date: "Statistical Programming with R"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

---

## Packages and functions used
```{r message=FALSE}
library(magrittr) # pipes
library(DAAG) # data sets and functions
```

- `glm()` Generalized linear models
- `predict()` Obtain predictions based on a model
- `confint()` Obtain confidence intervals for model parameters
- `coef()` Obtain a model's coefficients
- `DAAG::CVbinary()` Cross-validation for regression with a binary response


# Generalized linear models
## GLM's
We knew 
\[y=\alpha+\beta x+\epsilon\]

Now we consider 
\[\mathbb{E}[y] = \alpha + \beta x\]

They're the same. Different notation, different framework.

The upside is that we can now use a function for the expectation $\mathbb{E}$ to allow for transformations. This would enable us to change $\mathbb{E}[y]$ such that $f(\mathbb{E}[y])$ has a linear relation with $x$.

The function $f()$ we call the `link function`. This function tranforms the scale of the response/outcome to the scale of the linear predictor.

Examples: $f(x) = x, \\ f(x) = 1/x, \\ f(x) = \log(x), \\ f(x) = \log(x/(1-x)).$

## Link functions
![Overview of link functions, bluntly borrowed from Wikipedia](links.png)

## GLM's continued

Remember that
\[y=\alpha+\beta x+\epsilon,\]

and that 
\[\mathbb{E}[y] = \alpha + \beta x.\]

As a result
\[y = \mathbb{E}[y] + \epsilon.\]

and residuals do not need to be normal (heck, $y$ probably isn't, so why should $\epsilon$ be?)

## Logit
\[\text{logit}(p) = \log(\frac{p}{1-p}) = \log(p) - \log(1-p)\]
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
p <- (1:999)/1000 
gitp <- log(p/(1 - p)) 
par(pty="s")
plot(gitp, p, xlab = "logit(p)", ylab = "p", type = "l", pch = 1)
```

## Logit continued
Logit models work on the $\log(\text{odds})$ scale

\[\log(\text{odds}) = \log(\frac{p}{1-p}) = \log(p) - \log(1-p) = \text{logit}(p)\]

The logit of the probability is the log of the odds. 

Logistic regression allows us to model the $\log(\text{odds})$ as a function of other, linear predictors. 

## $\log(\text{odds})$ explained
```{r cache=TRUE,  dev.args = list(bg = 'transparent')}
logodds <- rep(NA, 1001)
for(i in 0:1000){
  p <- i / 1000 
  logodds[i + 1] <- log(p / (1 - p))
}
head(logodds)
tail(logodds)
```

## $\log(\text{odds})$ explained
```{r,  dev.args = list(bg = 'transparent')}
plot(0:1000, logodds, xlab = "x", ylab = "log(odds)", type = "l")
```

## logit$^{-1}$ explained
```{r cache=TRUE,  dev.args = list(bg = 'transparent')}
invlogit <- exp(logodds) / (1 + exp(logodds))
invlogit %>% head()
invlogit %>% head()
```

## logit$^{-1}$ explained
```{r,  dev.args = list(bg = 'transparent')}
plot(0:1000, invlogit, xlab = "x", ylab = "log(odds)", type = "l")
```

# Logistic regression
## Logistic regression
With linear regression we had the `Sum of Squares (SS)`. Its logistic counterpart is the `Deviance (D)`. 

 -  Deviance is the fit of the observed values to the expected values. 
 
With logistic regression we aim to maximize the `likelihood`, which is equivalent to minimizing the deviance. 

The likelihood is the (joint) probability of the observed values, given the current model parameters.

In normally distributed data: $\text{SS}=\text{D}$.

## Logistic regression
```{r, message = FALSE}
anesthetic %>% head(n = 18)
```

## Logistic regression
```{r,  dev.args = list(bg = 'transparent')}
anesthetic %$% glm(nomove ~ conc, family = binomial(link="logit")) %>% summary()
```

## A different approach
```{r,  dev.args = list(bg = 'transparent')}
anestot <- aggregate(anesthetic[, c("move","nomove")],  
                     by = list(conc = anesthetic$conc), FUN = sum) 
anestot
```

## A different approach
```{r,  dev.args = list(bg = 'transparent')}
anestot$total <- apply(anestot[, c("move","nomove")], 1 , sum) 
anestot$prop <- anestot$nomove / anestot$total 
anestot
```

## A different approach
```{r,  dev.args = list(bg = 'transparent')}
anestot %$% glm(prop ~ conc, family = binomial(link="logit"), weights = total) %>% 
  summary()
```

## Logistic multiple regression
Always try to make the relation as linear as possible 

- after all you are assessing a linear model. 

Do not forget that you use transformations to "make" things more linear

## Logistic multiple regression
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(distance, NoOfPools, NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("Distance", "NoOfPools", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Logistic multiple regression
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(log(distance), log(NoOfPools), NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("log(Distance)", "log(NoOfPools)", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Logistic multiple regression
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
summary(frogs.glm0 <- glm(formula = pres.abs ~ log(distance) + log(NoOfPools) 
                          + NoOfSites + avrain +  I(meanmax + meanmin) 
                          + I(meanmax - meanmin), family = binomial, data = frogs)) 
```

## Logistic multiple regression
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
frogs.glm <- frogs %$% glm(pres.abs ~ log(distance) + log(NoOfPools) 
                           + I(meanmax + meanmin) + I(meanmax - meanmin),  
                           family = binomial)
frogs.glm %>% summary()
```

## Fitted values
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm <- frogs %$% glm(pres.abs ~ log(distance) + log(NoOfPools) 
                           + I(meanmax + meanmin) + I(meanmax - meanmin),  
                           family = binomial)
frogs.glm %>% fitted() %>% head()
frogs.glm %>% predict(type = "response") %>% head()
frogs.glm %>% predict(type = "link") %>% head() # Scale of linear predictor 
```

## Fitted values with approximate SE's
```{r,  dev.args = list(bg = 'transparent')}
pred <- frogs.glm %>% 
  predict(type = "link", se.fit = TRUE)
data.frame(fit = pred$fit, se = pred$se.fit) %>% head()
```

## Confidence intervals for the $\beta$
```{r}
frogs.glm %>% confint()
frogs.glm %>% coef()
```

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
set.seed(123)
frogs.glm <- glm(pres.abs ~ log(distance) + log(NoOfPools), 
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm)
```

The cross-validation measure is the proportion of predictions over the folds that are correct. 

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm2 <- glm(pres.abs ~ log(distance) + log(NoOfPools) 
                 + I(meanmax + meanmin) + I(meanmax - meanmin),
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm2)
```

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm2 <- glm(pres.abs ~ log(distance) + log(NoOfPools) 
                 + I(meanmax + meanmin) + I(meanmax - meanmin),
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm2, nfolds = 5)
```



  